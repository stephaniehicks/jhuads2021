[
  {
    "path": "posts/2021-11-18-support-vector-machines/",
    "title": "Support vector machines",
    "description": "Introduction to support vector machines (SVMs)",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-11-18",
    "categories": [
      "machine learning",
      "support vector machines"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nAcknowledgements\n\nLearning objectives\nMotivation\nHow does it work?\nHyperplanes\nMaximal Margin Hyperplane (or Classifer)\nSupport Vector Classifer\nSupport Vector Classifier with Non-Linear boundaries\n\nSupport Vector Machines\nAdvantages\nDisadvantages\n\nData\nImport data\nWrangling data\nEDA\n\nClassification models\nSupport Vector Machines\nLogistic regression\nRandom Forests\nBagging and boosting\nChecking test error rate\n\nSummary\n\n\nPre-lecture materials\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nAn Introduction to Statistical Learning, 2nd edition by James, Witten, Hastie and Tibshirani\nhttps://jhu-advdatasci.github.io/2019/\nLearning objectives\n\nAt the end of this lesson you will:\nUnderstand what are hyperplanes, maximal margin hyperplanes, support vector classifiers (with and without linear boundaries), and Support Vector Machines (SVM)\nKnow how to build a SVM with the caret R package\n\nMotivation\nIn this lecture, we will be asking the question:\n\nCan we identify a voice as male or female, based upon acoustic properties of the voice and speech?\n\n\n\n\n[image source]\nDetermining a person’s gender as male or female, based upon a sample of their voice seems to initially be a feasible task. Often, the human ear can easily detect the difference between a male or female voice within the first few spoken words. However, designing a computer program to do this turns out to be a bit trickier.\nTo accomplish that goal, we will learn about another machine learning algorithm called support vector machines (SVMs). SVMs have been around since the 1990s and originated of the computer science community. They are form of supervised learning.\nSVMs are widely applied to pattern classification and regression problems, such as:\nHandwritten digits classification\nSpeech recognition\nFacial expression classification\nText classification\nThe original idea was to build a classifier for which training data can be separated using some type of linear hyperplane. We want a hyperplane that maximizes the distance between the hyperplane to the nearest data point in either class.\n\n\n\n[image source]\nIn the case when we cannot draw a linear hyperplane to separate the two classes of points (this is more typical), we can use adapt the idea and build a non-linear classifer. The key idea is to apply a “kernel trick”. We’ll learn more about that later in the lecture.\n\n\n\n[image source]\nNote: We will focus on the case when there are only two classes, but there are also extensions of SVMs in the case when there are more than two classes.\nHow does it work?\nGiven a dataset with a set of features and set of labels, we want to build a support vector machine (SVMs) to predict classes for new observations.\nTo understand what is a SVM let’s build up to it and consider some other types of classifiers (or hyperplanes) and how it relates to SVMs. First, let’s define what is a hyperplane.\nHyperplanes\nA hyperplane is formally defined as a flat affine subspace of a dimension \\(p-1\\). e.g. In two dimensions, a hyperplane is a flat one-dimensional subspace (or a line). In this case, a hyperplane is defined by\n\\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = 0 \\]\nfor \\(X = (X_1, X_2)^{T}\\) and for parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\beta_2\\). If there are \\(X = (X_1, X_2)^{T}\\) that do not satisify the above, i.e\n\\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 > 0 \\]\nor\n\\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 < 0 \\]\nThen, we can think of the hyperplane as dividing the two-dimensional space into two halves.\nIn the figure below, the hyperplane \\(1 + 2X_1 + 3X_2 = 0\\) is shown. The set of points in the blue region is \\(1 + 2X_1 + 3X_2 > 0\\) and the purple region is the set of points for which \\(1 + 2X_1 + 3X_2 < 0\\).\n\n\n\n[image source]\nMore formally, let’s say we have a set of \\(n\\) training observations \\(X_i = (X_{i1}, X_{i2})^T\\) in with two features (\\(p=2\\)) and each training observation has a known label \\(y_i \\in \\{-1,1\\}\\) where the observations from the blue class are labeled as \\(y_i = 1\\) and those from the purple class are \\(y_i = -1\\).\nA hyperplane that separates the observations\n\\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 > 0 \\text{ if } y_i = 1 \\]\nor\n\\[ \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 < 0 \\text{ if } y_i = -1 \\] or\n\\[ y_i (\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2) < 0 \\text{ for all } i\\in(1, \\ldots, n) \\]\nThere can be many hyperplanes that separate these points in our example. For example, the figure on the left shows three hyperplanes in black (out of many). If we pick one hyperplane, the figure on the right shows a grid of blue an purple points indicating the decision rule made by a classifer defined by this hyperplane.\n\n\n\n[image source]\nMore formally, we can classify a test observation \\(x^{*}\\) based on the sign of of\n\\[ f(x^{*}) = \\beta_0 + \\beta_1 x_1^{*} + \\beta_2 x_2^{*} \\]\nIf \\(f(x^{*})\\) is positive, then we assign \\(x^{*}\\) to the blue class.\nIf \\(f(x^{*})\\) is negative, then we assign \\(x^{*}\\) to the purple class.\nIn addition to the sign, we can also consider the magnitude of \\(f(x^{*})\\).\nIf \\(f(x^{*})\\) is far from zero, then \\(x^{*}\\) is far away from the hyperplane (i.e. more confidence in our class assignment).\nIf \\(f(x^{*})\\) is close to zero, then \\(x^{*}\\) is close to the hyperplane (i.e. less certain about the class assignment for \\(x^{*}\\)).\nBut, the problem is this still can lead to an infinite number of possible separating hyperplanes. How can we decide what is the “best” hyperplane?\nMaximal Margin Hyperplane (or Classifer)\nThe maximal margin hyperplane is the hyperlane that separates the farthest from training observations.\nIn the figure below, the maximal margin hyperplane is shown as a solid line. The margin is the distance from the solid line to either of the dashed lines. The two blue points and the purple point that lie on the dashed lines are the support vectors (they “support” the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well), and the distance from those points to the margin is indicated by arrows. The purple and blue grid indicates the decision rule made by a classifier based on this separating hyperplane.\n\n\n\n[image source]\nNote: although the maximal margin classifier is often successful, it can also lead to overfitting when \\(p\\) is large. So we want to explore the use of cross-validation.\nTo construct a maximal margin classifier using \\(n\\) training observations \\(x_1, \\ldots, x_n \\in \\mathbb{R}^p\\) and associated class labels \\(y_1 \\ldots, y_n \\in \\{-1, 1\\}\\), the maximal margin hyperplane is the solution to the optimization problem:\n\\[ \\underset{\\beta_0, \\beta_1, \\ldots, \\beta_p, M}{\\text{maximize}} M \\] subject to \\(\\sum_{j=1}^p \\beta_j^2 = 1\\) and\n\\[ y_i (\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p) \\geq M \\text{ for all } i\\in(1, \\ldots, n) \\]\nThis guarantees that each observation will be on the correct side of the hyperplane and at least a distance of \\(M\\) from the hyperplane. Therefore, you can think of \\(M\\) as the margin of our hyperplane.\nThis works great if a separating hyperplane exists. However, many times, that isn’t true and there is no solution with \\(M > 0\\). So instead we can try to find a hyperplane that almost separates the classes.\nSupport Vector Classifer\nConsider the following data that cannot be separated by a hyperplane.\n\n\n\n[image source]\nWe could consider building a support vector classifer or a soft margin classifer that misclassifies a few training observations in order to do a better job of classifying the remaining observations.\nThe margin is soft because it can be violated by some of the training observations. An observation can be not only on the wrong side of the margin, but also on the wrong side of the hyperplane.\n\n\n\n[image source]\nOn the left there are observations that are on the right side of the hyperplane, but the wrong side of the margin. On the right are observations that are on the wrong side of the hyperplane and the wrong side of the margin.\nIn fact, when there is no separating hyperplane, such a situation is inevitable. Observations on the wrong side of the hyperplane correspond to training observations that are misclassified by the support vector classifier (i.e. right figure above).\nNow, the optimization problem is:\n\\[ \\underset{\\beta_0, \\beta_1, \\ldots, \\beta_p, \\epsilon_1, \\ldots, \\epsilon_n, M}{\\text{maximize}} M \\] subject to \\(\\sum_{j=1}^p \\beta_j^2 = 1\\)\n\\[ y_i (\\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_p x_p) \\geq M (1-\\epsilon_i)  \\] for all \\(i\\in(1, \\ldots, n)\\), \\(\\epsilon_i \\geq 0\\), \\(\\sum_{i=1}^n \\epsilon_i \\leq C\\) where \\(C\\) is a nonnegative tuning parameter (typically chosen using cross-validation). The \\(\\epsilon_1, \\ldots, \\epsilon_n\\) are often called slack variables that allow observations to be on the wrong side of the margin or hyperplane.\nComments on the tuning parameter \\(C\\)\nWe see that \\(C\\) bounds the sum of the \\(\\epsilon_i\\)’s, so you think about it as a “budget” for the amount of margin that can be violated in the \\(n\\) observations.\nIf \\(C = 0\\) \\(\\Rightarrow\\) No budget for violations to the margin and \\(\\epsilon_1 = \\ldots = \\epsilon_n = 0\\). This is essentially the maximal margin hyperplane (but only if the two classes are separable).\nIf \\(C > 0\\) \\(\\Rightarrow\\) No more than \\(C\\) observations can be on the wrong side of the hyperplane, because if an observation is on the wrong side of the hyperplane then \\(\\epsilon_i > 1\\), and we saw above, this requires that \\(\\sum_{i=1}^n \\epsilon_i \\leq C\\).\nIn general, as the budget \\(C\\) increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as \\(C\\) decreases, we become less tolerant of violations to the margin and so the margin narrows.\nAnother way of thinking about \\(C\\) is that it basically controls the bias-variance trade-off.\nWhen \\(C\\) is small, we seek narrow margins that are rarely violated; this amounts to a classifier that is highly fit to the data, which may have low bias but high variance.\nWhen \\(C\\) is larger, the margin is wider and we allow more violations to it; this amounts to fitting the data less hard and obtaining a classifier that is potentially more biased but may have lower variance.\nInterestingly, it turns out that only the observations that lie on the margin or that violate the margin (also known as support vectors) will affect the hyperplane (and hence classification).\nThis make sense. When \\(C\\) is large, the margin is wide, and many observations violate the margin, thus there are many support vectors (potentially more bias, but less variance). When \\(C\\) is small, the margin is small, not many observations violate the margin, thus very few support vectors (potentially low bias and high variance).\n\n\n\n[image source]\nBut what if we want to consider non-linear boundaries?\nSupport Vector Classifier with Non-Linear boundaries\nThus far, the support vector classifier has been very useful for classification in the setting with two classes and if the classes can be separated by a linear hyperplane (with or without some violations of margins of error). However, more often than not, the boundry will need to be more flexible and consider non-linear class boundaries.\nConsider the following data on the left plot. A linear support classifier (applied in the right plot) will perform poorly.\n\n\n\n[image source]\nWhen using linear regression, if there is a non-linear relationship between the predictos and outcome, a solution is to enlarge the feature space to include e.g. quadratic and cubic terms\nTherefore, a solution to our problem is to enlarge the feature space using functions of the predictors (i.e.  quadratic and cubic terms or higher) in order to address the non-linearity.\nSo instead of fitting a support vector classifier with \\(p\\) features \\((X_1, X_2, \\ldots, X_p)\\), we could try using \\(2p\\) features \\((X_1, X_1^2, X_2, X_2^2, \\ldots, X_p, X_p^2)\\).\nNow, the optimization problem becomes:\n\\[ \\underset{\\beta_0, \\beta_{11}, \\beta_{12}, \\ldots, \\beta_{p1}, \\beta_{p2}, \\epsilon_1, \\ldots, \\epsilon_n, M}{\\text{maximize}} M \\] subject to\n\\[ y_i (\\beta_0 + \\sum_{j=1}^p \\beta_{j1} x_{ij} + \\sum_{j=1}^p \\beta_{j2} x_{ij}^2) \\geq M (1-\\epsilon_i)  \\] \\(\\sum_{i=1}^n \\epsilon_i \\leq C\\), \\(\\epsilon_i \\geq 0\\), \\(\\sum_{j=1}^p \\sum_{k=1}^2 \\beta_{jk}^2 = 1\\)\nWhy does this lead to a non-linear boundary?\nIn the enlarged feature space, the decision boundary that is found is still linear. But in the original feature space, the decision boundary is of the form \\(q(x) = 0\\), where \\(q\\) is a quadratic polynomial, and its solutions are generally non-linear.\nAs you can imagine, there are many ways to enlarge the feature space e.g. include higher-order polynomial terms or interaction terms such as \\(X_1 X_2\\). We could easily end up with a large number of features leading to unmanagable computations.\nIn the next section, we will learn about the support vector machine that allows us to enlarge the feature space in an efficient way.\nSupport Vector Machines\nThe support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels.\nThe details of how exactly how the support vector classifier is computed is quite technical, so I won’t go into it here. However, it’s sufficient to know that the solution to the support vector classifier problem involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two observations \\(x_i\\) and \\(x_{i^{'}}\\) is given by\n\\[ \\langle x_i, x_{i^{'}} \\rangle = \\sum_{j=1}^P x_{ij} x_{i^{'}j}\\]\nFor example, the linear support vector classifier can be represented as\n\\[ f(x) = \\beta_0 + \\sum_{i=1}^n \\alpha_i \\langle x, x_{i} \\rangle \\]\nwhere there are \\(n\\) parameters \\(\\alpha_i\\) (one per training observation).\nTo estimate the parameters \\(\\alpha_1, \\ldots, \\alpha_n\\) and \\(\\beta_0\\), we need to take \\({n \\choose 2}\\) inner products between all pairs of training observations \\(x_i\\) and \\(x_{i^{'}}\\).\nTo evaluate the function \\(f(x)\\), all we need is the inner product between a new point \\(x\\) and each of the training points \\(x_i\\). Interestingly, it turns out that \\(\\alpha_i\\) is nonzero only for the support vectors (points in the margin). Another way of stating this is if a training observation is not a support vector, then its \\(\\alpha_i\\) is equals zero.\nSo if we consider \\(\\mathcal{S}\\) as the collection of indicies for support points, then we can rewrite the above to\n\\[ f(x) = \\beta_0 + \\sum_{i \\in \\mathcal{S}} \\alpha_i \\langle x, x_{i} \\rangle \\] And this involves a lot fewer points!\nOk, now suppose that instead of the inner product, we consider a generalization of the inner product of the form\n\\[ K( x_i, x_{i^{'}} ) \\]\nwhere \\(K\\) is some function called a kernel.\nYou can think of a kernel as a function that quantifies the similiarity of two observations. For example,\n\\[ K( x_i, x_{i^{'}} ) = \\sum_{j=1}^p x_{ij} x_{i^{'}j} \\]\nis a linear kernel (linear in the features) and would return the support vector classifier. In contrast, this kernel is called a polynomial kernel of degree \\(d\\). If \\(d > 1\\), then the support vector classifier results in a more flexible boundaary.\n\\[ K( x_i, x_{i^{'}} ) = \\Big(1 + \\sum_{j=1}^p x_{ij} x_{i^{'}j} \\Big)^d \\]\n\n\n\n[image source]\nWhen the support vector classifier is combined with non-linear kernels (such as above), the resulting classifier is known as a support vector machine.\nAnother popular kernel is the radial kernel:\n\\[ K( x_i, x_{i^{'}} ) = \\exp \\Big(-\\gamma \\sum_{j=1}^p (x_{ij} - x_{i^{'}j})^2 \\Big) \\]\n\n\n\n[image source]\nAdvantages\nSVMs are effective when the number of features is quite large.\nIt works effectively even if the number of features are greater than the number of samples.\nNon-Linear data can also be classified using customized hyperplanes built by using kernel trick.\nIt is a robust model to solve prediction problems since it maximizes margin.\nDisadvantages\nThe biggest limitation of SVMs is the choice of the kernel. The wrong choice of the kernel can lead to an increase in error percentage.\nWith a greater number of samples, it can result in poor performance.\nSVMs have good generalization performance but they can be extremely slow in the test phase.\nSVMs have high algorithmic complexity and extensive memory requirements due to the use of quadratic programming.\nLet’s try out these concepts on the data from our original question:\n\nCan we identify a voice as male or female, based upon acoustic properties of the voice and speech?\n\nData\nThe data we will use is from kaggle and is available in a .csv file.\nA description of the data from Kaggle:\n\n“This database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages.”\n\nWe can actually dig a bit deeper and go to the website where the data origianlly came from to learn more about how the dataset was created:\n\n“Each voice sample is stored as a .WAV file, which is then pre-processed for acoustic analysis using the specan function from the WarbleR R package. Specan measures 22 acoustic parameters on acoustic signals for which the start and end times are provided.”\n\n\n“The output from the pre-processed WAV files were saved into a CSV file, containing 3168 rows and 21 columns (20 columns for each feature and one label column for the classification of male or female).”\n\nThe following acoustic properties of each voice are measured (described on Kaggle’s website):\nVariable\nDescription\nmeanfreq\nmean frequency (in kHz)\nsd\nstandard deviation of frequency\nmedian\nmedian frequency (in kHz)\nQ25\nfirst quantile (in kHz)\nQ75\nthird quantile (in kHz)\nIQR\ninterquantile range (in kHz)\nskew\nskewness\nkurt\nkurtosis\nsp.ent\nspectral entropy\nsfm\nspectral flatness\nmode\nmode frequency\ncentroid\nfrequency centroid\npeakf\npeak frequency (frequency with highest energy)\nmeanfun\naverage of fundamental frequency measured across acoustic signal\nminfun\nminimum fundamental frequency measured across acoustic signal\nmaxfun\nmaximum fundamental frequency measured across acoustic signal\nmeandom\naverage of dominant frequency measured across acoustic signal\nmindom\nminimum of dominant frequency measured across acoustic signal\nmaxdom\nmaximum of dominant frequency measured across acoustic signal\ndfrange\nrange of dominant frequency measured across acoustic signal\nmodindx\nmodulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\nlabel\nmale or female\nImport data\nFirst, we load a few R packages\n\n\nlibrary(here)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(kernlab)\nlibrary(GGally)\n\n\n\nLet’s read in the voice.csv file into R using the read_csv() function in the readr R package.\n\n\nvoice <- read_csv(here(\"data\", \"voice.csv\"))\nvoice\n\n\n# A tibble: 3,168 × 21\n   meanfreq     sd median     Q25    Q75    IQR  skew    kurt sp.ent\n      <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl> <dbl>   <dbl>  <dbl>\n 1   0.0598 0.0642 0.0320 0.0151  0.0902 0.0751 12.9   274.    0.893\n 2   0.0660 0.0673 0.0402 0.0194  0.0927 0.0733 22.4   635.    0.892\n 3   0.0773 0.0838 0.0367 0.00870 0.132  0.123  30.8  1025.    0.846\n 4   0.151  0.0721 0.158  0.0966  0.208  0.111   1.23    4.18  0.963\n 5   0.135  0.0791 0.125  0.0787  0.206  0.127   1.10    4.33  0.972\n 6   0.133  0.0796 0.119  0.0680  0.210  0.142   1.93    8.31  0.963\n 7   0.151  0.0745 0.160  0.0929  0.206  0.113   1.53    5.99  0.968\n 8   0.161  0.0768 0.144  0.111   0.232  0.121   1.40    4.77  0.959\n 9   0.142  0.0780 0.139  0.0882  0.209  0.120   1.10    4.07  0.971\n10   0.134  0.0804 0.121  0.0756  0.202  0.126   1.19    4.79  0.975\n# … with 3,158 more rows, and 12 more variables: sfm <dbl>,\n#   mode <dbl>, centroid <dbl>, meanfun <dbl>, minfun <dbl>,\n#   maxfun <dbl>, meandom <dbl>, mindom <dbl>, maxdom <dbl>,\n#   dfrange <dbl>, modindx <dbl>, label <chr>\n\nNext, let’s get an overall summary of the range of values in the dataset.\n\n\nsummary(voice)\n\n\n    meanfreq             sd              median       \n Min.   :0.03936   Min.   :0.01836   Min.   :0.01097  \n 1st Qu.:0.16366   1st Qu.:0.04195   1st Qu.:0.16959  \n Median :0.18484   Median :0.05916   Median :0.19003  \n Mean   :0.18091   Mean   :0.05713   Mean   :0.18562  \n 3rd Qu.:0.19915   3rd Qu.:0.06702   3rd Qu.:0.21062  \n Max.   :0.25112   Max.   :0.11527   Max.   :0.26122  \n      Q25                 Q75               IQR         \n Min.   :0.0002288   Min.   :0.04295   Min.   :0.01456  \n 1st Qu.:0.1110865   1st Qu.:0.20875   1st Qu.:0.04256  \n Median :0.1402864   Median :0.22568   Median :0.09428  \n Mean   :0.1404556   Mean   :0.22476   Mean   :0.08431  \n 3rd Qu.:0.1759388   3rd Qu.:0.24366   3rd Qu.:0.11418  \n Max.   :0.2473469   Max.   :0.27347   Max.   :0.25223  \n      skew              kurt              sp.ent      \n Min.   : 0.1417   Min.   :   2.068   Min.   :0.7387  \n 1st Qu.: 1.6496   1st Qu.:   5.670   1st Qu.:0.8618  \n Median : 2.1971   Median :   8.319   Median :0.9018  \n Mean   : 3.1402   Mean   :  36.569   Mean   :0.8951  \n 3rd Qu.: 2.9317   3rd Qu.:  13.649   3rd Qu.:0.9287  \n Max.   :34.7255   Max.   :1309.613   Max.   :0.9820  \n      sfm               mode           centroid      \n Min.   :0.03688   Min.   :0.0000   Min.   :0.03936  \n 1st Qu.:0.25804   1st Qu.:0.1180   1st Qu.:0.16366  \n Median :0.39634   Median :0.1866   Median :0.18484  \n Mean   :0.40822   Mean   :0.1653   Mean   :0.18091  \n 3rd Qu.:0.53368   3rd Qu.:0.2211   3rd Qu.:0.19915  \n Max.   :0.84294   Max.   :0.2800   Max.   :0.25112  \n    meanfun            minfun             maxfun      \n Min.   :0.05557   Min.   :0.009775   Min.   :0.1031  \n 1st Qu.:0.11700   1st Qu.:0.018223   1st Qu.:0.2540  \n Median :0.14052   Median :0.046110   Median :0.2712  \n Mean   :0.14281   Mean   :0.036802   Mean   :0.2588  \n 3rd Qu.:0.16958   3rd Qu.:0.047904   3rd Qu.:0.2775  \n Max.   :0.23764   Max.   :0.204082   Max.   :0.2791  \n    meandom             mindom             maxdom         \n Min.   :0.007812   Min.   :0.004883   Min.   : 0.007812  \n 1st Qu.:0.419828   1st Qu.:0.007812   1st Qu.: 2.070312  \n Median :0.765795   Median :0.023438   Median : 4.992188  \n Mean   :0.829211   Mean   :0.052647   Mean   : 5.047277  \n 3rd Qu.:1.177166   3rd Qu.:0.070312   3rd Qu.: 7.007812  \n Max.   :2.957682   Max.   :0.458984   Max.   :21.867188  \n    dfrange          modindx           label          \n Min.   : 0.000   Min.   :0.00000   Length:3168       \n 1st Qu.: 2.045   1st Qu.:0.09977   Class :character  \n Median : 4.945   Median :0.13936   Mode  :character  \n Mean   : 4.995   Mean   :0.17375                     \n 3rd Qu.: 6.992   3rd Qu.:0.20918                     \n Max.   :21.844   Max.   :0.93237                     \n\nA quick glimpse over the data shows us that we have 20 numeric columns with differing ranges and magnitudes.\nWrangling data\nIt would be nice to get a picture of how these features are different across the male and female observations. One way to do that is to use ggplot() to explore differences in distribution with boxplots and histograms.\nFirst, let’s transform the data from a wide format to a long format using the gather() function in the tidyr package.\n\n\nvoice_long <- voice %>% \n  gather(key = feature, value = value, -label)\nhead(voice_long)\n\n\n# A tibble: 6 × 3\n  label feature   value\n  <chr> <chr>     <dbl>\n1 male  meanfreq 0.0598\n2 male  meanfreq 0.0660\n3 male  meanfreq 0.0773\n4 male  meanfreq 0.151 \n5 male  meanfreq 0.135 \n6 male  meanfreq 0.133 \n\nWe also can transform the label column which contains male and female character strings into 1s and 0s where 1 represents male and 0 represents `female.\n\n\ntable(voice$label)\n\n\n\nfemale   male \n  1584   1584 \n\n\n\nvoice_labels <- voice$label\n\nvoice <- voice %>% \n  mutate(y = factor(ifelse(label==\"male\",1,0) )) %>% \n  select(-label)\n\n\n\nJust as a sanity check:\n\n\ntable(voice_labels, voice$y)\n\n\n            \nvoice_labels    0    1\n      female 1584    0\n      male      0 1584\n\nWhew ok good!\nEDA\nIf we wanted to create boxplots of all twenty variables colored by whether the observation was male or female, we can use the\n\n\nvoice_long %>%\n    ggplot(aes(label, value, colour = label)) + \n        geom_boxplot(alpha = 0.5) + \n        facet_wrap(~ feature, scales='free_y', ncol = 4) + \n        labs(x = NULL, y = NULL) + \n        theme_minimal()\n\n\n\n\n\n\nvoice_long %>%\n    ggplot(aes(value, fill = label)) + \n        geom_density(alpha = 0.5) + \n        facet_wrap(~ feature, scales='free', ncol = 4) + \n        labs(x = NULL, y = NULL) + \n        theme_minimal()\n\n\n\n\nThese are great to look at the distributions separately, but it would also be good to get an idea of how the features are related to each other.\nTo do that, another useful plotting function for exploratory data analysi is the ggpairs() function from the GGally package:\n\n\nvoice %>% \n  select(IQR, meanfun, Q25, sd, sfm, sp.ent, y) %>%\n  ggpairs(ggplot2::aes(colour=factor(y)))\n\n\n\n\nClassification models\nNext, we will build a few models to classify the recorded voice samples as male or female using features available. First, we will look at SVMs and then we will compare to other models that we have already seen that are useful for classification including logistic regression and random forests.\nSupport Vector Machines\nBefore we build an SVM classifier, let’s split our data into a train_set and test_set using the createDataParition() function in the caret R package.\nWe’ll just split it in half for the purposes of the lecture.\n\n\nset.seed(1234)\nset.seed(1234)\ntrain_set = createDataPartition(y = voice$y, \n                                p = 0.5, list=FALSE)\n\ntrain_dat = voice[train_set,]\ntest_dat = voice[-train_set,]\n\n\n\nWe can look at the dimensions of the two datasets to make sure they have been split in half.\n\n\ndim(train_dat)\n\n\n[1] 1584   21\n\ndim(test_dat)\n\n\n[1] 1584   21\n\nAnd they have! Ok, before we build a SVM using train() function (we’ve seen this before), let’s use the trainControl() function. Here, we select method=cv with 10-fold cross-validation.\n\n\ncontrol <- trainControl(method=\"cv\", number=10)\nmetric <- \"Accuracy\"\n\n\n\nSVM with linear kernel\nFirst, we will use the train() function from the caret R package with the argument method=svmLinear to build a SVM with linear kernel.\n\n\nfit_svmLinear <- train(y~., data=train_dat, method=\"svmLinear\",\n                metric=metric, trControl=control)\nfit_svmLinear\n\n\nSupport Vector Machines with Linear Kernel \n\n1584 samples\n  20 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1424, 1426, 1426, 1425, 1426, 1426, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.9722233  0.9444468\n\nTuning parameter 'C' was held constant at a value of 1\n\n\n\nnewdata <- as.data.frame(select(train_dat, -y))\npred_svmLinear <- predict(fit_svmLinear, newdata)\nconfusionMatrix(reference=train_dat$y, pred_svmLinear)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 775  19\n         1  17 773\n                                         \n               Accuracy : 0.9773         \n                 95% CI : (0.9687, 0.984)\n    No Information Rate : 0.5            \n    P-Value [Acc > NIR] : <2e-16         \n                                         \n                  Kappa : 0.9545         \n                                         \n Mcnemar's Test P-Value : 0.8676         \n                                         \n            Sensitivity : 0.9785         \n            Specificity : 0.9760         \n         Pos Pred Value : 0.9761         \n         Neg Pred Value : 0.9785         \n             Prevalence : 0.5000         \n         Detection Rate : 0.4893         \n   Detection Prevalence : 0.5013         \n      Balanced Accuracy : 0.9773         \n                                         \n       'Positive' Class : 0              \n                                         \n\nSVM with polynomial kernel\nNext, we will use the train() function from the caret R package with the argument method=svmPoly to build a SVM with polynomial kernel.\n\n\nfit_svmPoly <- train(y~., data=train_dat, method=\"svmPoly\",\n                metric=metric, trControl=control)\nfit_svmPoly\n\n\nSupport Vector Machines with Polynomial Kernel \n\n1584 samples\n  20 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1425, 1425, 1426, 1426, 1426, 1425, ... \nResampling results across tuning parameters:\n\n  degree  scale  C     Accuracy   Kappa    \n  1       0.001  0.25  0.8876682  0.7752831\n  1       0.001  0.50  0.8965130  0.7929541\n  1       0.001  1.00  0.9123079  0.8245470\n  1       0.010  0.25  0.9318884  0.8637212\n  1       0.010  0.50  0.9564923  0.9129594\n  1       0.010  1.00  0.9653372  0.9306550\n  1       0.100  0.25  0.9684818  0.9369517\n  1       0.100  0.50  0.9684738  0.9369354\n  1       0.100  1.00  0.9728923  0.9457742\n  2       0.001  0.25  0.8965130  0.7929541\n  2       0.001  0.50  0.9116750  0.8232812\n  2       0.001  1.00  0.9262081  0.8523590\n  2       0.010  0.25  0.9634344  0.9268506\n  2       0.010  0.50  0.9672160  0.9344180\n  2       0.010  1.00  0.9691028  0.9381925\n  2       0.100  0.25  0.9690829  0.9381582\n  2       0.100  0.50  0.9716105  0.9432131\n  2       0.100  1.00  0.9722474  0.9444863\n  3       0.001  0.25  0.9078776  0.8156823\n  3       0.001  0.50  0.9205278  0.8409935\n  3       0.001  1.00  0.9413622  0.8826757\n  3       0.010  0.25  0.9672160  0.9344182\n  3       0.010  0.50  0.9691028  0.9381941\n  3       0.010  1.00  0.9678370  0.9356624\n  3       0.100  0.25  0.9690789  0.9381492\n  3       0.100  0.50  0.9716066  0.9432057\n  3       0.100  1.00  0.9747632  0.9495179\n\nAccuracy was used to select the optimal model using the\n largest value.\nThe final values used for the model were degree = 3, scale = 0.1\n and C = 1.\n\n\n\nnewdata <- as.data.frame(select(train_dat, -y))\npred_svmPoly <- predict(fit_svmPoly, newdata)\nconfusionMatrix(reference=train_dat$y, pred_svmPoly)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 789   3\n         1   3 789\n                                          \n               Accuracy : 0.9962          \n                 95% CI : (0.9918, 0.9986)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9924          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9962          \n            Specificity : 0.9962          \n         Pos Pred Value : 0.9962          \n         Neg Pred Value : 0.9962          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4981          \n   Detection Prevalence : 0.5000          \n      Balanced Accuracy : 0.9962          \n                                          \n       'Positive' Class : 0               \n                                          \n\nSVM with radial basis kernel\nNext, we will use the train() function from the caret R package with the argument method=svmRadial to build a SVM with radial basis kernel.\n\n\nfit_svmRadial <- train(y~., data=train_dat, method=\"svmRadial\",\n                       metric=metric, trControl=control)\nfit_svmRadial\n\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n1584 samples\n  20 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1426, 1424, 1426, 1426, 1426, 1426, ... \nResampling results across tuning parameters:\n\n  C     Accuracy   Kappa    \n  0.25  0.9658863  0.9317722\n  0.50  0.9722114  0.9444228\n  1.00  0.9747391  0.9494778\n\nTuning parameter 'sigma' was held constant at a value of 0.05522729\nAccuracy was used to select the optimal model using the\n largest value.\nThe final values used for the model were sigma = 0.05522729 and C = 1.\n\n\n\nnewdata <- as.data.frame(select(train_dat, -y))\npred_svmRadial <- predict(fit_svmRadial, newdata)\nconfusionMatrix(reference=train_dat$y, pred_svmRadial)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 784  16\n         1   8 776\n                                          \n               Accuracy : 0.9848          \n                 95% CI : (0.9775, 0.9903)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9697          \n                                          \n Mcnemar's Test P-Value : 0.153           \n                                          \n            Sensitivity : 0.9899          \n            Specificity : 0.9798          \n         Pos Pred Value : 0.9800          \n         Neg Pred Value : 0.9898          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4949          \n   Detection Prevalence : 0.5051          \n      Balanced Accuracy : 0.9848          \n                                          \n       'Positive' Class : 0               \n                                          \n\nLogistic regression\nNow, just for fun, let’s compare to some other classification approaches that we have previously learned about.\nFirst, let’s try logistic regression.\n\n\nfit_glm <- train(y ~ ., data = train_dat, trControl = control, \n                 method = 'glm', family = 'binomial')\nfit_glm\n\n\nGeneralized Linear Model \n\n1584 samples\n  20 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1426, 1425, 1425, 1425, 1425, 1426, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.9634026  0.9268036\n\n\n\nnewdata <- as.data.frame(select(train_dat, -y))\npred_glm <- predict(fit_glm, newdata)\nconfusionMatrix(reference=train_dat$y, pred_glm)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 770  20\n         1  22 772\n                                          \n               Accuracy : 0.9735          \n                 95% CI : (0.9643, 0.9808)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.947           \n                                          \n Mcnemar's Test P-Value : 0.8774          \n                                          \n            Sensitivity : 0.9722          \n            Specificity : 0.9747          \n         Pos Pred Value : 0.9747          \n         Neg Pred Value : 0.9723          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4861          \n   Detection Prevalence : 0.4987          \n      Balanced Accuracy : 0.9735          \n                                          \n       'Positive' Class : 0               \n                                          \n\nThat’s actually not so bad.\nRandom Forests\nNext let’s try random forests.\n\n\nfit_rf <- train(y~., data=train_dat, method=\"rf\", \n                metric=metric, trControl=control)\n\n\n\n\n\nnewdata <- as.data.frame(select(train_dat, -y))\npred_rf <- predict(fit_rf, newdata)\nconfusionMatrix(reference=train_dat$y, pred_rf)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 792   0\n         1   0 792\n                                     \n               Accuracy : 1          \n                 95% CI : (0.9977, 1)\n    No Information Rate : 0.5        \n    P-Value [Acc > NIR] : < 2.2e-16  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n                                     \n            Sensitivity : 1.0        \n            Specificity : 1.0        \n         Pos Pred Value : 1.0        \n         Neg Pred Value : 1.0        \n             Prevalence : 0.5        \n         Detection Rate : 0.5        \n   Detection Prevalence : 0.5        \n      Balanced Accuracy : 1.0        \n                                     \n       'Positive' Class : 0          \n                                     \n\nI’m forgetting where the performance of a random forest model compared to everyone else. So let’s take a closer look.\n\n\nclass_results <- resamples(list(glm=fit_glm, rf=fit_rf, \n                  fit_svmLinear=fit_svmLinear, fit_svmPoly=fit_svmPoly, \n                  fit_svmRadial=fit_svmRadial))\nsummary(class_results)\n\n\n\nCall:\nsummary.resamples(object = class_results)\n\nModels: glm, rf, fit_svmLinear, fit_svmPoly, fit_svmRadial \nNumber of resamples: 10 \n\nAccuracy \n                   Min.   1st Qu.    Median      Mean   3rd Qu.\nglm           0.9493671 0.9511882 0.9621646 0.9634026 0.9731510\nrf            0.9493671 0.9637867 0.9715986 0.9715747 0.9811022\nfit_svmLinear 0.9430380 0.9684042 0.9746835 0.9722233 0.9794304\nfit_svmPoly   0.9559748 0.9653690 0.9779277 0.9747632 0.9811022\nfit_svmRadial 0.9493671 0.9699367 0.9749214 0.9747391 0.9810127\n                   Max. NA's\nglm           0.9873418    0\nrf            0.9937107    0\nfit_svmLinear 0.9937107    0\nfit_svmPoly   0.9873418    0\nfit_svmRadial 0.9874214    0\n\nKappa \n                   Min.   1st Qu.    Median      Mean   3rd Qu.\nglm           0.8987342 0.9024092 0.9243240 0.9268036 0.9463027\nrf            0.8987342 0.9275622 0.9431962 0.9431465 0.9621966\nfit_svmLinear 0.8860759 0.9368090 0.9493671 0.9444468 0.9588608\nfit_svmPoly   0.9119114 0.9307089 0.9558544 0.9495179 0.9622011\nfit_svmRadial 0.8987342 0.9398734 0.9498418 0.9494778 0.9620253\n                   Max. NA's\nglm           0.9746835    0\nrf            0.9874199    0\nfit_svmLinear 0.9874219    0\nfit_svmPoly   0.9746835    0\nfit_svmRadial 0.9748418    0\n\n\n\ndotplot(class_results)\n\n\n\n\nSo it looks like SVM does give us a bit of a performance boost over logistic regression or random forests.\nBagging and boosting\nWhat about bagging or boosting? Here, we try comparing existing results to both bagging and boosting (what we learned previously in class).\n\n\nfit_treebag <- train(y~., data=train_dat, method=\"treebag\", \n                metric=metric, trControl=control)\n\nfit_boost <- train(y~., data=train_dat, method=\"gbm\", \n                metric=metric, trControl=control, verbose = FALSE)\n\n\n\n\n\n# summarize results\nclass_results <- resamples(list(glm=fit_glm, rf=fit_rf, \n                  fit_svmLinear=fit_svmLinear, fit_svmPoly=fit_svmPoly, \n                  fit_svmRadial=fit_svmRadial,\n                  fit_treebag = fit_treebag,\n                  fit_boost = fit_boost))\n\n\n\n\n\ndotplot(class_results)\n\n\n\n\nChecking test error rate\nOK, so now that I have selected the SVM (Poly) classifier as the one that I will use (built on our train_dat), we can classify the recorded voice samples in our test_dat using the predict() function.\n\n\nnewdata <- as.data.frame(select(test_dat, -y))\npred_svmPoly_test <- predict(fit_svmPoly, newdata)\nconfusionMatrix(reference=test_dat$y, pred_svmPoly_test)\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 778  22\n         1  14 770\n                                         \n               Accuracy : 0.9773         \n                 95% CI : (0.9687, 0.984)\n    No Information Rate : 0.5            \n    P-Value [Acc > NIR] : <2e-16         \n                                         \n                  Kappa : 0.9545         \n                                         \n Mcnemar's Test P-Value : 0.2433         \n                                         \n            Sensitivity : 0.9823         \n            Specificity : 0.9722         \n         Pos Pred Value : 0.9725         \n         Neg Pred Value : 0.9821         \n             Prevalence : 0.5000         \n         Detection Rate : 0.4912         \n   Detection Prevalence : 0.5051         \n      Balanced Accuracy : 0.9773         \n                                         \n       'Positive' Class : 0              \n                                         \n\nSummary\nStill need to write this part.\n\n\n\n",
    "preview": "http://acoustics.org/pressroom/httpdocs/162nd/Images/Elliot_Figure%2014%20-%20Frequency%20ranges%20for%20common%20sounds.jpg",
    "last_modified": "2021-11-16T09:37:28-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-16-tidytext-and-sentiment-analysis/",
    "title": "Tidytext and sentiment analysis",
    "description": "Introduction to tidytext and sentiment analysis",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-11-16",
    "categories": [
      "tidyverse",
      "tidytext",
      "sentiment analysis"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nAcknowledgements\n\nLearning objectives\nMotivation\nTidy text\nWhat is a tidy format?\nWhat is this common philosphy?\nWhat is a tidy text format?\nWhy is this format useful?\nHow does it work?\nText Mining and Tokens\n\nExample: text from works of Jane Austen\nSentiment Analysis\nThe sentiments dataset\nJoining together tidy text data with lexicons\nWord clouds\n\n\nConverting to and from tidy and non-tidy formats\n\n\nPre-lecture materials\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nText mining with R: A Tidy Approach from Julia Silge and David Robinson which uses the tidytext R package\nSupervised Machine Learning for Text Analsyis in R from Emil Hvitfeldt, Julia Sigle\nLearning objectives\n\nAt the end of this lesson you will:\nLearn about what is is meant by “tidy text” data\nKnow the fundamentals of the texttidy R package to create tidy text data\nKnow the fundamentals of sentiment analysis\n\nMotivation\nAnalyzing text data such as Twitter content, books or news articles is commonly performed in data science.\nIn this lecture, we will be asking the following questions:\n\nWhich are the most commonly used words from Jane Austen’s novels? Which are the most positive or negative words? How does the sentiment (e.g. positive vs negative) of the text change across each novel?\n\n\n\n\n[image source]\nTo answer these questions, we will need to learn about a few things. Specifically,\nHow to convert words in documents to a tidy text format using the tidytext R package\nA little bit about sentiment analysis\nTidy text\nIn previous courses, you have learned about the tidy data principles and the tidyverse R packages as a way to make handling data easier and more effective. These packages depend on data being formatted in a particular way. The idea with tidy text is to treat text as data frames of individual words and apply the same tidy data principles to make text mining tasks easier and consistent with already developed tools.\nFirst let’s recall what a tidy data format means.\nWhat is a tidy format?\nFirst, the tidyverse is\n\n“an opinionated collection of R packages designed for data science. All packages share an underlying philosophy and common APIs.”\n\nAnother way of putting it is that it is a set of packages that are useful specifically for data manipulation, exploration and visualization with a common philosophy.\nWhat is this common philosphy?\nThe common philosophy is called “tidy” data. It is a standard way of mapping the meaning of a dataset to its structure.\nIn tidy data:\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\nWorking with tidy data is useful because it creates a structured way of organizing data values within a data set. This makes the data analysis process more efficient and simplifies the development of data analysis tools that work together.\nIn this way, you can focus on the problem you are investigating, rather than the uninteresting logistics of data.\nWhat is a tidy text format?\nWhen dealing with text data, the tidy text format is defined as a table with one-token-per-row, where a token is a meaningful unit of text (e.g. a word, pair of words, sentence, paragraph, etc).\nUsing a given set of token, we can tokenize text, or split the text into the defined tokens of interest along the rows. We will learn more about how to do this using functions in the tidytext R package.\nIn contrast, other data structures that are commonly used to store text data in text mining applications:\nstring: text can, of course, be stored as strings, i.e., character vectors, within R, and often text data is first read into memory in this form.\ncorpus: these types of objects typically contain raw strings annotated with additional metadata and details.\ndocument-term matrix: This is a sparse matrix describing a collection (i.e., a corpus) of documents with one row for each document and one column for each term. The value in the matrix is typically word count.\nI won’t describing these other formats in greater detail, but encourage you to read about them if interested in this topic.\nWhy is this format useful?\nOne of the biggest advantages of transforming text data to the tidy text format is that it allows data to transition smoothly between other packages that adhere to the tidyverse framework (e.g. ggplot2, dplyr, etc).\n\n\n\nFigure 1: A flowchart of a typical text analysis using tidy data principles.\n\n\n\n[image source]\nIn addition, a user can transition between the tidy text format for e.g data visualization with ggplot2, but then also convert data to other data structures (e.g. document-term matrix) that is commonly used in machine learning applications.\nHow does it work?\nThe main workhorse function in the tidytext R package to tokenize text a data frame is the unnest_tokens(tbl, output, input) function.\n\n\n?unnest_tokens\n\n\n\nIn addition to the data frame (tbl), the function needs two basic arguments:\noutput or the output column name that will be created (e.g. string) as the text is unnested into it\ninput or input column name that the text comes from and gets split\nLet’s try out the unnest_tokens() function using the first paragraph in the preface of Roger’s R Programming for Data Science book.\nFirst, we load a few R packages\n\n\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(tidytext) ## needs to be installed\nlibrary(janeaustenr) ## needs to be installed\n\n\n\nAnd then we load in the text\n\n\npeng_preface <- \n  c(\"I started using R in 1998 when I was a college undergraduate working on my senior thesis.\", \n    \"The version was 0.63.\",  \n    \"I was an applied mathematics major with a statistics concentration and I was working with Dr. Nicolas Hengartner on an analysis of word frequencies in classic texts (Shakespeare, Milton, etc.).\", \n    \"The idea was to see if we could identify the authorship of each of the texts based on how frequently they used certain words.\", \n    \"We downloaded the data from Project Gutenberg and used some basic linear discriminant analysis for the modeling.\",\n    \"The work was eventually published and was my first ever peer-reviewed publication.\", \n    \"I guess you could argue it was my first real 'data science' experience.\")\n\npeng_preface\n\n\n[1] \"I started using R in 1998 when I was a college undergraduate working on my senior thesis.\"                                                                                                        \n[2] \"The version was 0.63.\"                                                                                                                                                                            \n[3] \"I was an applied mathematics major with a statistics concentration and I was working with Dr. Nicolas Hengartner on an analysis of word frequencies in classic texts (Shakespeare, Milton, etc.).\"\n[4] \"The idea was to see if we could identify the authorship of each of the texts based on how frequently they used certain words.\"                                                                    \n[5] \"We downloaded the data from Project Gutenberg and used some basic linear discriminant analysis for the modeling.\"                                                                                 \n[6] \"The work was eventually published and was my first ever peer-reviewed publication.\"                                                                                                               \n[7] \"I guess you could argue it was my first real 'data science' experience.\"                                                                                                                          \n\nTurns out Roger performed a similar analysis as an undergraduate student!\nHe goes to say that back then no one was using R (but a little bit of something called S-PLUS), so I can only imagine how different it was to accomplish a task like the one we are going to do today compared to when he was an undergraduate.\nHere we use the tibble() function to construct a data frame from the character strings in peng_preface.\n\n\npeng_preface_df <- tibble(line=1:7, text=peng_preface)\npeng_preface_df\n\n\n# A tibble: 7 × 2\n   line text                                                          \n  <int> <chr>                                                         \n1     1 I started using R in 1998 when I was a college undergraduate …\n2     2 The version was 0.63.                                         \n3     3 I was an applied mathematics major with a statistics concentr…\n4     4 The idea was to see if we could identify the authorship of ea…\n5     5 We downloaded the data from Project Gutenberg and used some b…\n6     6 The work was eventually published and was my first ever peer-…\n7     7 I guess you could argue it was my first real 'data science' e…\n\nText Mining and Tokens\nNext, we will use the unnest_tokens() function where we will call the output column to be created word and the input column text from the peng_preface_df.\n\n\npeng_token <- \n  peng_preface_df %>% \n  unnest_tokens(output = word, input = text, token = \"words\")\n\npeng_token %>% head()\n\n\n# A tibble: 6 × 2\n   line word   \n  <int> <chr>  \n1     1 i      \n2     1 started\n3     1 using  \n4     1 r      \n5     1 in     \n6     1 1998   \n\npeng_token %>% tail()\n\n\n# A tibble: 6 × 2\n   line word      \n  <int> <chr>     \n1     7 my        \n2     7 first     \n3     7 real      \n4     7 data      \n5     7 science   \n6     7 experience\n\nThe argument token=\"words\" defines the unit for tokenization. The default is \"words\", but there are lots of other options.\nFor example, we could tokenize by \"characters\":\n\n\npeng_preface_df %>% \n  unnest_tokens(word, text, token = \"characters\") %>% \n  head()\n\n\n# A tibble: 6 × 2\n   line word \n  <int> <chr>\n1     1 i    \n2     1 s    \n3     1 t    \n4     1 a    \n5     1 r    \n6     1 t    \n\nor something called ngrams, which is defined by Wikipedia as a “contiguous sequence of n items from a given sample of text or speech”\n\n\npeng_preface_df %>% \n  unnest_tokens(word, text, token = \"ngrams\", n=3) %>% \n  head()\n\n\n# A tibble: 6 × 2\n   line word           \n  <int> <chr>          \n1     1 i started using\n2     1 started using r\n3     1 using r in     \n4     1 r in 1998      \n5     1 in 1998 when   \n6     1 1998 when i    \n\nAnother option is to use the character_shingles option, which is similar to tokenizing like ngrams, except the units are characters instead of words.\n\n\npeng_preface_df %>% \n  unnest_tokens(word, text, token = \"character_shingles\", n = 4) %>% \n  head()\n\n\n# A tibble: 6 × 2\n   line word \n  <int> <chr>\n1     1 ista \n2     1 star \n3     1 tart \n4     1 arte \n5     1 rted \n6     1 tedu \n\nYou can also create custom functions for tokenization.\n\n\npeng_preface_df %>% \n  unnest_tokens(word, text, token = stringr::str_split, pattern = \" \") %>% \n  head()\n\n\n# A tibble: 6 × 2\n   line word   \n  <int> <chr>  \n1     1 i      \n2     1 started\n3     1 using  \n4     1 r      \n5     1 in     \n6     1 1998   \n\nExample: text from works of Jane Austen\nWe will use the text from six published novels from Jane Austen, which are available in the janeaustenr R package. The authors describe the format:\n\n\"The package provides the text in a one-row-per-line format, where a line is this context is analogous to a literal printed line in a physical book.\nThe package contains:\nsensesensibility: Sense and Sensibility, published in 1811\nprideprejudice: Pride and Prejudice, published in 1813\nmansfieldpark: Mansfield Park, published in 1814\nemma: Emma, published in 1815\nnorthangerabbey: Northanger Abbey, published posthumously in 1818\npersuasion: Persuasion, also published posthumously in 1818\nThere is also a function austen_books() that returns a tidy data frame of all 6 novels.\"\n\nLet’s load in the text from prideprejudice and look at how the data are stored.\n\n\nlibrary(janeaustenr)\nhead(prideprejudice, 20)\n\n\n [1] \"PRIDE AND PREJUDICE\"                                                        \n [2] \"\"                                                                           \n [3] \"By Jane Austen\"                                                             \n [4] \"\"                                                                           \n [5] \"\"                                                                           \n [6] \"\"                                                                           \n [7] \"Chapter 1\"                                                                  \n [8] \"\"                                                                           \n [9] \"\"                                                                           \n[10] \"It is a truth universally acknowledged, that a single man in possession\"    \n[11] \"of a good fortune, must be in want of a wife.\"                              \n[12] \"\"                                                                           \n[13] \"However little known the feelings or views of such a man may be on his\"     \n[14] \"first entering a neighbourhood, this truth is so well fixed in the minds\"   \n[15] \"of the surrounding families, that he is considered the rightful property\"   \n[16] \"of some one or other of their daughters.\"                                   \n[17] \"\"                                                                           \n[18] \"\\\"My dear Mr. Bennet,\\\" said his lady to him one day, \\\"have you heard that\"\n[19] \"Netherfield Park is let at last?\\\"\"                                         \n[20] \"\"                                                                           \n\nWe see each line is in a character vector with elements of about 70 characters.\nSimilar to what we did above with Roger’s preface, we can turn the text of character strings into a data frame and then convert it into a one-row-per-line dataframe using the unnest_tokens() function.\n\n\npp_book_df <- tibble(text = prideprejudice) \n  \npp_book_df %>% \n  unnest_tokens(word, text, token=\"words\")\n\n\n# A tibble: 122,204 × 1\n   word     \n   <chr>    \n 1 pride    \n 2 and      \n 3 prejudice\n 4 by       \n 5 jane     \n 6 austen   \n 7 chapter  \n 8 1        \n 9 it       \n10 is       \n# … with 122,194 more rows\n\nWe can also divide it by paragraphs:\n\n\ntmp <- pp_book_df %>% \n  unnest_tokens(output = paragraph, input = text, token=\"paragraphs\") \nhead(tmp)\n\n\n# A tibble: 6 × 1\n  paragraph                                                           \n  <chr>                                                               \n1 pride and prejudice                                                 \n2 by jane austen                                                      \n3 chapter 1                                                           \n4 it is a truth universally acknowledged, that a single man in posses…\n5 of a good fortune, must be in want of a wife.                       \n6 however little known the feelings or views of such a man may be on …\n\ntmp[5,1]\n\n\n# A tibble: 1 × 1\n  paragraph                                    \n  <chr>                                        \n1 of a good fortune, must be in want of a wife.\n\nNote: what you name the output column, e.g. paragraph in this case, doesn’t affect it, it’s just good to give it a consistent name.\nWe could also divide it by sentence:\n\n\npp_book_df %>%\n    unnest_tokens(output = sentence, input = text, token = \"sentences\") \n\n\n# A tibble: 15,545 × 1\n   sentence                                                           \n   <chr>                                                              \n 1 \"pride and prejudice\"                                              \n 2 \"by jane austen\"                                                   \n 3 \"chapter 1\"                                                        \n 4 \"it is a truth universally acknowledged, that a single man in poss…\n 5 \"of a good fortune, must be in want of a wife.\"                    \n 6 \"however little known the feelings or views of such a man may be o…\n 7 \"first entering a neighbourhood, this truth is so well fixed in th…\n 8 \"of the surrounding families, that he is considered the rightful p…\n 9 \"of some one or other of their daughters.\"                         \n10 \"\\\"my dear mr.\"                                                    \n# … with 15,535 more rows\n\nNote: this is tricked by terms like “Mr.” and “Mrs.”\nOne neat trick is that we can unnest by two layers: paragraph and then word. This lets us keep track of which paragraph is which.\n\n\nparagraphs <- \n  pp_book_df %>%\n    unnest_tokens(output = paragraph, input = text, token = \"paragraphs\") %>%\n    mutate(paragraph_number = row_number()) \n\nparagraphs\n\n\n# A tibble: 10,721 × 2\n   paragraph                                          paragraph_number\n   <chr>                                                         <int>\n 1 \"pride and prejudice\"                                             1\n 2 \"by jane austen\"                                                  2\n 3 \"chapter 1\"                                                       3\n 4 \"it is a truth universally acknowledged, that a s…                4\n 5 \"of a good fortune, must be in want of a wife.\"                   5\n 6 \"however little known the feelings or views of su…                6\n 7 \"first entering a neighbourhood, this truth is so…                7\n 8 \"of the surrounding families, that he is consider…                8\n 9 \"of some one or other of their daughters.\"                        9\n10 \"\\\"my dear mr. bennet,\\\" said his lady to him one…               10\n# … with 10,711 more rows\n\nNote: We use mutate() to annotate a paragraph number quantity to keep track of paragraphs in the original format.\n\n\nparagraphs %>%\n    unnest_tokens(output = word, input = paragraph)\n\n\n# A tibble: 122,204 × 2\n   paragraph_number word     \n              <int> <chr>    \n 1                1 pride    \n 2                1 and      \n 3                1 prejudice\n 4                2 by       \n 5                2 jane     \n 6                2 austen   \n 7                3 chapter  \n 8                3 1        \n 9                4 it       \n10                4 is       \n# … with 122,194 more rows\n\nWe notice there are many what are called stop words (“the”, “of”, “to”, and so forth in English). Often in text analysis, we will want to remove stop words because stop words are words that are not useful for an analysis. We can remove stop words (kept in the tidytext dataset stop_words) with an anti_join().\n\n\ndata(stop_words)\n\ntable(stop_words$lexicon)\n\n\n\n    onix    SMART snowball \n     404      571      174 \n\nstop_words %>% \n  head(n=10)\n\n\n# A tibble: 10 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n\n\n\nwords_by_paragraph <- \n  paragraphs %>%\n    unnest_tokens(word, paragraph) %>%\n    anti_join(stop_words)\n\nwords_by_paragraph \n\n\n# A tibble: 37,246 × 2\n   paragraph_number word        \n              <int> <chr>       \n 1                1 pride       \n 2                1 prejudice   \n 3                2 jane        \n 4                2 austen      \n 5                3 chapter     \n 6                3 1           \n 7                4 truth       \n 8                4 universally \n 9                4 acknowledged\n10                4 single      \n# … with 37,236 more rows\n\nBecause we have stored our data in a tidy dataset, we can use tidyverse packages for exploratory data analysis.\nFor example, here we use dplyr’s count() function to find the most common words in the book\n\n\nwords_by_paragraph %>%\n  count(word, sort = TRUE) %>% \n  head()\n\n\n# A tibble: 6 × 2\n  word          n\n  <chr>     <int>\n1 elizabeth   597\n2 darcy       373\n3 bennet      294\n4 miss        283\n5 jane        264\n6 bingley     257\n\nThen use ggplot2 to plot the most commonly used words from the book.\n\n\nwords_by_paragraph %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 150) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip()\n\n\n\n\nWe can also do this for all of her books using the austen_books() object\n\n\nausten_books() %>% \n  head()\n\n\n# A tibble: 6 × 2\n  text                    book               \n  <chr>                   <fct>              \n1 \"SENSE AND SENSIBILITY\" Sense & Sensibility\n2 \"\"                      Sense & Sensibility\n3 \"by Jane Austen\"        Sense & Sensibility\n4 \"\"                      Sense & Sensibility\n5 \"(1811)\"                Sense & Sensibility\n6 \"\"                      Sense & Sensibility\n\nWe can do some data wrangling that keep tracks of the line number and chapter (using a regex) to find where all the chapters are.\n\n\noriginal_books <- \n  austen_books() %>%\n  group_by(book) %>%\n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divxlc]\",\n                                                 ignore_case = TRUE)))) %>%\n  ungroup()\n\noriginal_books\n\n\n# A tibble: 73,422 × 4\n   text                    book                linenumber chapter\n   <chr>                   <fct>                    <int>   <int>\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility          1       0\n 2 \"\"                      Sense & Sensibility          2       0\n 3 \"by Jane Austen\"        Sense & Sensibility          3       0\n 4 \"\"                      Sense & Sensibility          4       0\n 5 \"(1811)\"                Sense & Sensibility          5       0\n 6 \"\"                      Sense & Sensibility          6       0\n 7 \"\"                      Sense & Sensibility          7       0\n 8 \"\"                      Sense & Sensibility          8       0\n 9 \"\"                      Sense & Sensibility          9       0\n10 \"CHAPTER 1\"             Sense & Sensibility         10       1\n# … with 73,412 more rows\n\nFinally, we can restructure it to a one-token-per-row format using the unnest_tokens() function and remove stop words using the anti_join() function in dplyr.\n\n\ntidy_books <- original_books %>%\n  unnest_tokens(word, text) %>% \n  anti_join(stop_words)\n\ntidy_books\n\n\n# A tibble: 217,609 × 4\n   book                linenumber chapter word       \n   <fct>                    <int>   <int> <chr>      \n 1 Sense & Sensibility          1       0 sense      \n 2 Sense & Sensibility          1       0 sensibility\n 3 Sense & Sensibility          3       0 jane       \n 4 Sense & Sensibility          3       0 austen     \n 5 Sense & Sensibility          5       0 1811       \n 6 Sense & Sensibility         10       1 chapter    \n 7 Sense & Sensibility         10       1 1          \n 8 Sense & Sensibility         13       1 family     \n 9 Sense & Sensibility         13       1 dashwood   \n10 Sense & Sensibility         13       1 settled    \n# … with 217,599 more rows\n\nHere are the most commonly used words across all of Jane Austen’s books.\n\n\ntidy_books %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 600) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(word, n)) +\n  geom_col() +\n  xlab(NULL) +\n  coord_flip()\n\n\n\n\nSentiment Analysis\nIn the previous section, we explored the tidy text format and showed how we can calculate things such as word frequency.\nNext, we are going to look at something called opinion mining or sentiment analysis. The tidytext authors write:\n\n“When human readers approach a text, we use our understanding of the emotional intent of words to infer whether a section of text is positive or negative, or perhaps characterized by some other more nuanced emotion like surprise or disgust. We can use the tools of text mining to approach the emotional content of text programmatically, as shown in the figure below”\n\n\n\n\nFigure 2: A flowchart of a typical text analysis that uses tidytext for sentiment analysis.\n\n\n\n[image source]\n\n“One way to analyze the sentiment of a text is to consider the text as a combination of its individual words and the sentiment content of the whole text as the sum of the sentiment content of the individual words. This isn’t the only way to approach sentiment analysis, but it is an often-used approach, and an approach that naturally takes advantage of the tidy tool ecosystem.”\n\nLet’s try using sentiment analysis on the Jane Austen books.\nThe sentiments dataset\nInside the tidytext package are several sentiment lexicons. A few things to note:\nThe lexicons are based on unigrams (single words)\nThe lexicons contain many English words and the words are assigned scores for positive/negative sentiment, and also possibly emotions like joy, anger, sadness, and so forth\nYou can use the get_sentiments() function to extract a specific lexicon.\nThe nrc lexicon categorizes words into multiple categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust\n\n\nget_sentiments(\"nrc\")\n\n\n# A tibble: 13,901 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 abacus      trust    \n 2 abandon     fear     \n 3 abandon     negative \n 4 abandon     sadness  \n 5 abandoned   anger    \n 6 abandoned   fear     \n 7 abandoned   negative \n 8 abandoned   sadness  \n 9 abandonment anger    \n10 abandonment fear     \n# … with 13,891 more rows\n\nThe bing lexicon categorizes words in a binary fashion into positive and negative categories\n\n\nget_sentiments(\"bing\")\n\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   <chr>       <chr>    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# … with 6,776 more rows\n\nThe AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment\n\n\nget_sentiments(\"afinn\")\n\n\n# A tibble: 2,477 × 2\n   word       value\n   <chr>      <dbl>\n 1 abandon       -2\n 2 abandoned     -2\n 3 abandons      -2\n 4 abducted      -2\n 5 abduction     -2\n 6 abductions    -2\n 7 abhor         -3\n 8 abhorred      -3\n 9 abhorrent     -3\n10 abhors        -3\n# … with 2,467 more rows\n\nThe authors of the tidytext package note:\n\n“How were these sentiment lexicons put together and validated? They were constructed via either crowdsourcing (using, for example, Amazon Mechanical Turk) or by the labor of one of the authors, and were validated using some combination of crowdsourcing again, restaurant or movie reviews, or Twitter data. Given this information, we may hesitate to apply these sentiment lexicons to styles of text dramatically different from what they were validated on, such as narrative fiction from 200 years ago. While it is true that using these sentiment lexicons with, for example, Jane Austen’s novels may give us less accurate results than with tweets sent by a contemporary writer, we still can measure the sentiment content for words that are shared across the lexicon and the text.”\n\nTwo other caveats:\n\n“Not every English word is in the lexicons because many English words are pretty neutral. It is important to keep in mind that these methods do not take into account qualifiers before a word, such as in”no good\" or “not true”; a lexicon-based method like this is based on unigrams only. For many kinds of text (like the narrative examples below), there are not sustained sections of sarcasm or negated text, so this is not an important effect.\"\n\nand\n\n“One last caveat is that the size of the chunk of text that we use to add up unigram sentiment scores can have an effect on an analysis. A text the size of many paragraphs can often have positive and negative sentiment averaged out to about zero, while sentence-sized or paragraph-sized text often works better.”\n\nJoining together tidy text data with lexicons\nNow that we have our data in a tidy text format and we have learned about different types of lexicons in application for sentiment analysis, we can join the words together using a join function.\nFor example, what are the most common joy words in the book Emma? Here we will use the nrc lexicon and join the tidy_books dataset with the nrc_joy lexicon using the inner_join() function in dplyr.\n\n\nnrc_joy <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"joy\")\n\ntidy_books %>%\n  filter(book == \"Emma\") %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE)\n\n\n# A tibble: 298 × 2\n   word          n\n   <chr>     <int>\n 1 friend      166\n 2 hope        143\n 3 happy       125\n 4 love        117\n 5 deal         92\n 6 found        92\n 7 happiness    76\n 8 pretty       68\n 9 true         66\n10 comfort      65\n# … with 288 more rows\n\nWe can do things like investigate how the sentiment of the text changes throughout each of Jane’s novels.\nHere we will use the bing lexicon, find a sentiment score for each word, and then use inner_join().\n\n\ntidy_books %>%\n  inner_join(get_sentiments(\"bing\")) %>% \n  head()\n\n\n# A tibble: 6 × 5\n  book                linenumber chapter word        sentiment\n  <fct>                    <int>   <int> <chr>       <chr>    \n1 Sense & Sensibility         16       1 respectable positive \n2 Sense & Sensibility         18       1 advanced    positive \n3 Sense & Sensibility         20       1 death       negative \n4 Sense & Sensibility         21       1 loss        negative \n5 Sense & Sensibility         25       1 comfortably positive \n6 Sense & Sensibility         28       1 goodness    positive \n\nThen we can count how many positive and negative words there are in each section of the books. We create an index to help us keep track of where we are in the narrative, which uses integer division, and counts up sections of 80 lines of text.\n\n\ntidy_books %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(book, index = linenumber %/% 80, sentiment) \n\n\n# A tibble: 1,840 × 4\n   book                index sentiment     n\n   <fct>               <dbl> <chr>     <int>\n 1 Sense & Sensibility     0 negative     16\n 2 Sense & Sensibility     0 positive     26\n 3 Sense & Sensibility     1 negative     19\n 4 Sense & Sensibility     1 positive     44\n 5 Sense & Sensibility     2 negative     12\n 6 Sense & Sensibility     2 positive     23\n 7 Sense & Sensibility     3 negative     15\n 8 Sense & Sensibility     3 positive     22\n 9 Sense & Sensibility     4 negative     16\n10 Sense & Sensibility     4 positive     29\n# … with 1,830 more rows\n\nNote: The %/% operator does integer division (x %/% y is equivalent to floor(x/y)) so the index keeps track of which 80-line section of text we are counting up negative and positive sentiment in.\nFinally, we use spread() to have positive and negative counts in different columns, and then use mutate() to calculate a net sentiment (positive - negative).\n\n\njane_austen_sentiment <- \n  tidy_books %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(book, index = linenumber %/% 80, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative)\n\njane_austen_sentiment\n\n\n# A tibble: 920 × 5\n   book                index negative positive sentiment\n   <fct>               <dbl>    <dbl>    <dbl>     <dbl>\n 1 Sense & Sensibility     0       16       26        10\n 2 Sense & Sensibility     1       19       44        25\n 3 Sense & Sensibility     2       12       23        11\n 4 Sense & Sensibility     3       15       22         7\n 5 Sense & Sensibility     4       16       29        13\n 6 Sense & Sensibility     5       16       39        23\n 7 Sense & Sensibility     6       24       37        13\n 8 Sense & Sensibility     7       22       39        17\n 9 Sense & Sensibility     8       30       35         5\n10 Sense & Sensibility     9       14       18         4\n# … with 910 more rows\n\nThen we can plot the sentiment scores across the sections of each novel:\n\n\nggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~book, ncol = 2, scales = \"free_x\")\n\n\n\n\nWe can see how the sentiment trajectory of the novel changes over time.\nWord clouds\nYou can also do things like create word clouds using the wordcloud package.\n\n\nlibrary(wordcloud)\n\ntidy_books %>%\n  anti_join(stop_words) %>%\n  count(word) %>%\n  with(wordcloud(word, n, max.words = 100))\n\n\n\n\nConverting to and from tidy and non-tidy formats\nhttps://www.tidytextmining.com/dtm.html\n\n\n\n",
    "preview": "https://images-na.ssl-images-amazon.com/images/I/A1YUH7-W5AL.jpg",
    "last_modified": "2021-11-16T09:31:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-10-linear-classification/",
    "title": "Linear Classification",
    "description": "Introduction to linear classification methods.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-11-07",
    "categories": [
      "machine learning",
      "linear classification"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nAcknowledgements\n\nLearning objectives\nMotivation\nThree widely used linear classifiers\n\nData\nData import\nWhat are the features?\nWhat are the outcomes?\nHow does this related to machine learning?\nCalculating the top PCs\n\n\nData wrangling\nFeature engineering\nExploratory data analysis\nCreate train_set and test_set\n\nLinear methods for classification\nLinear regression\nLogistic regression\nLinear discriminant analysis\n\\(K\\)-nearest neighbors\n\n\nPre-lecture materials\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nAn Introduction to Statistical Learning, 2013 by James, Witten, Hastie and Tibshirani\nThe dsbook from Rafael Irizarry\nhttps://en.wikipedia.org/wiki/Linear_discriminant_analysis\nLearning objectives\n\nAt the end of this lesson you will:\nKnow how to implement three linear classifiers: (i) linear and logistic regression, (ii) linear and quadratic discriminant analysis, and (iii) \\(K\\)-nearest neighbors\n\nMotivation\nIn the previous lectures, we have learned about machine learning algorithms, where decisions made are based on algorithms built on data. The data we have often comes in the form of an outcome we want to predict and the features that we will use to predict the outcome. This setting is often called a supervised learning (as opposed to the unsupervised learning setting without an outcome variable, such as clustering or dimensionality reduction).\n\nThe general idea of supervised learning is that we build the algorithm using the data that includes the outcome so that in the future we can predict the outcome only using the features. Here we will use \\(Y\\) to denote the outcome and \\(X_1, \\dots, X_p\\) to denote features.\nNote: the features are sometimes referred to as predictors or covariates and the outcome is sometimes referred to as a response variable.\n\nIf the outcome \\(Y\\) is quantitative, something like linear regression is very useful for predicting a quantitative response. But in many situations, the responsible variable is qualitative (or categorical).\nPredicting a qualitative response \\(Y\\) is referred to as classification since it involves assigning each observation to a category or class.\nSo the general set-up is as follows. Given a set of predictors \\(X_{ij}\\) and and qualitative outcome \\(Y_i\\), we can collect data to classify or predict which class or category each outcome (or observation) belongs in:\n\noutcome\nfeature_1\nfeature_2\nfeature_3\nfeature_4\nfeature_5\nY_1\nX_1,1\nX_1,2\nX_1,3\nX_1,4\nX_1,5\nY_2\nX_2,1\nX_2,2\nX_2,3\nX_2,4\nX_2,5\nY_3\nX_3,1\nX_3,2\nX_3,3\nX_3,4\nX_3,5\nY_4\nX_4,1\nX_4,2\nX_4,3\nX_4,4\nX_4,5\nY_5\nX_5,1\nX_5,2\nX_5,3\nX_5,4\nX_5,5\nY_6\nX_6,1\nX_6,2\nX_6,3\nX_6,4\nX_6,5\nY_7\nX_7,1\nX_7,2\nX_7,3\nX_7,4\nX_7,5\nY_8\nX_8,1\nX_8,2\nX_8,3\nX_8,4\nX_8,5\nY_9\nX_9,1\nX_9,2\nX_9,3\nX_9,4\nX_9,5\nY_10\nX_10,1\nX_10,2\nX_10,3\nX_10,4\nX_10,5\n\nThree widely used linear classifiers\nToday, we will focus on the three of the most widely used classifiers:\nLinear and Logistic regression\nLinear and Quadratic discriminant analysis\n\\(K\\)-nearest neighbors\nYou have already learned about other methods including, trees, random forests, and boosting. Next week, we will cover support vector machines.\nData\nFor this lecture, we will use the Fashion-MNIST dataset from Kaggle.\nThe motivating question is:\nCan we build a classier to accurately classify images of pieces of clothing?\nThe data consists of a training set of 60,000 images and a test set of 10,000 examples. We will assume the test set is the only data available for the purposes of this lecture (mostly because it is 1/6 of the size of the training set!).\nFor example, we want to build a classifier to recognize this image as a pair of pants:\n\n\n\nAnd the classifier should be able to recognize this image as shoe:\n\n\n\nThe fashion MNIST dataset contains a set of images of clothing or fashion pieces. Each observation \\(Y_i\\) is labeled one of the following:\n0 = T-shirt/top\n1 = Trouser\n2 = Pullover\n3 = Dress\n4 = Coat\n5 = Sandal\n6 = Shirt\n7 = Sneaker\n8 = Bag\n9 = Ankle boot\nThe are images are converted into \\(28 \\times 28\\) pixels and for each we obtain an gray scale intensity between 0 (white) to 256 (black).\nWe will explore this data set using some common machine learning algorithms for classification.\nData import\nFirst, we load a few R packages\n\n\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(caret)\nlibrary(pROC)\nlibrary(here)\n\n\n\nThe data are available in the /data folder of this repository.\nFirst, let’s read in the fashion-mnist_test.csv dataset using the read_csv() function in the readr package.\n\n\nif(!exists(\"fashion\")) { \n  fashion <- read_csv(here(\"data\", \"fashion-mnist_test.csv\"))\n}\n\n\n\nHere we save just pixels\n\n\nX <- \n  fashion %>% \n  select(-label) %>% \n  as.matrix()\ndim(X)\n\n\n[1] 10000   784\n\nAnd we also save the labels (i.e. 0-9)\n\n\nfashion_labels <- \n  fashion %>% \n  pull(label)\nfashion_labels[1:5]\n\n\n[1] 0 1 2 2 3\n\nWe can look at the first three images to see what they look like.\n\n\ntmp <- lapply(c(1:3), function(i){\n    expand.grid(Row=1:28, Column=1:28) %>%  \n      mutate(id=i, label=fashion$label[i],  \n             value = unlist(fashion[i,-1])) })\ntmp <- Reduce(rbind, tmp)\ntmp %>% ggplot(aes(Row, Column, fill=value)) + \n    geom_raster() + \n    scale_y_reverse() +\n    scale_fill_gradient(low=\"white\", high=\"black\") +\n    facet_grid(.~label)\n\n\n\n\nWhat are the features?\nEach image is converted into \\(28 \\times 28\\) pixels and for each we obtain an grey scale intensity between 0 (white) to 255 (black).\nThis means one image has 784 (=28*28) features.\nWe can see these values like this:\n\n\ntmp %>% ggplot(aes(Row, Column, fill=value)) + \n    geom_point(pch=21,cex=2) + \n    scale_y_reverse() +\n    scale_fill_gradient(low=\"white\", high=\"black\") +\n    facet_grid(.~label)\n\n\n\n\nWe use bold face to denote this vector of predictors\n\\[ \\mathbf{X}_i = (X_{i,1}, \\dots, X_{i,784}) \\]\nLet’s take a peek at \\(\\mathbf{X}\\):\n\n\nX[1:10, 1:10]\n\n\n      pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 pixel9\n [1,]      0      0      0      0      0      0      0      9      8\n [2,]      0      0      0      0      0      0      0      0      0\n [3,]      0      0      0      0      0      0     14     53     99\n [4,]      0      0      0      0      0      0      0      0      0\n [5,]      0      0      0      0      0      0      0      0      0\n [6,]      0      0      0      0      0     44    105     44     10\n [7,]      0      0      0      0      0      0      0      0      0\n [8,]      0      0      0      0      0      0      0      1      0\n [9,]      0      0      0      0      0      0      0      0      0\n[10,]      0      0      0      0      0      0      0      0      0\n      pixel10\n [1,]       0\n [2,]       0\n [3,]      17\n [4,]     161\n [5,]       0\n [6,]       0\n [7,]       0\n [8,]       0\n [9,]       0\n[10,]       0\n\nWhat are the outcomes?\nEven though the label here is a whole number between 0 and 9, this is a qualitative outcome (e.g. shirt, pants, shoes, etc).\n\n\nfashion_labels[1:7]\n\n\n[1] 0 1 2 2 3 2 8\n\nSo for each image \\(i\\) we have an outcome \\(Y_i\\) which can be one of 10 categories: \\(0,1,2,3,4,5,6,7,8,9\\) and the features \\(X_{i,1}, \\dots, X_{i,784}\\) which can take values from 0 to 255.\nHow does this related to machine learning?\nThe machine learning task here is to build a predictor function, \\(f\\) that converts \\(\\mathbf{X}\\) into a prediction category \\(\\hat{Y}_i = f(\\mathbf{X}_i)\\) that minimizes the \\(d(Y,\\hat{Y})\\)\nCalculating the top PCs\nIf you recall, the first PC is will explain the most variation, the second PC will explain the second most variation in the data, etc.\nBecause the pixels are so small we expect those to be close to each other on the grid to be correlated, meaning that dimension reduction should be possible.\nLet’s take the singular value decomposition (SVD) of \\(\\mathbf{X}\\).\n\n\ndim(X) #  10000 observation, 784 features\n\n\n[1] 10000   784\n\nRemember, we need to column center the data (using scale()). We also will create a new variable \\(\\mathbf{Y}\\) to represent the standardized data that is also transposed (features along rows).\n\n\nY <- t(scale(X,center=TRUE, scale=FALSE)) \ndim(Y)\n\n\n[1]   784 10000\n\nNow apply the svd() function to \\(\\mathbf{Y}\\).\n\n\ns <- svd(Y)\nstr(s)\n\n\nList of 3\n $ d: num [1:784] 113672 89269 51516 47079 41325 ...\n $ u: num [1:784, 1:784] -7.63e-08 -1.01e-05 -4.32e-05 -5.51e-05 -1.87e-04 ...\n $ v: num [1:10000, 1:784] -0.012246 0.000349 0.007291 -0.006896 -0.00233 ...\n\nFirst note that we can in fact reconstruct \\(\\mathbf{Y}\\) using all the PCs:\n\n\nYhat <- s$u %*% diag(s$d) %*% t(s$v)\nresid <- Y - Yhat\nmax(abs(resid))\n\n\n[1] 6.843166e-10\n\nIf we look at the eigenvalues in \\(\\mathbf{D}\\), we see that the last few are quite close to 0.\n\n\nplot(s$d)\n\n\n\n\nThis implies that the last columns of \\(\\mathbf{V}\\) (right singular vectors) have a very small effect on the reconstruction of \\(\\mathbf{X}\\).\nTo see this, consider the extreme example in which the last entry of \\(\\mathbf{V}\\) is 0. In this case, the last column of \\(\\mathbf{V}\\) is not needed at all.\nBecause of the way the SVD is created, the columns of \\(\\mathbf{V}\\), have less and less influence on the reconstruction of \\(\\mathbf{X}\\). You commonly see this described as “explaining less variance”. This implies that for a large matrix, by the time you get to the last columns, it is possible that there is not much left to “explain”.\nAs an example, we will look at what happens if we remove the 100 last columns:\n\n\nk <- ncol(s$v)-100\nYhat <- s$u[,1:k] %*% diag(s$d)[1:k,1:k] %*% t(s$v[,1:k])\nresid <- Y - Yhat \nmax(abs(resid))\n\n\n[1] 182.8126\n\nThe largest residual is practically 0, meaning that Yhat is practically the same as Y, yet we need 100 less dimensions to transmit the information.\nBy looking at \\(\\mathbf{D}\\), we can see that, in this particular dataset, we can obtain a good approximation keeping only a subset of columns. The following plots are useful for seeing how much of the variability is explained by each column:\n\n\nplot(s$d^2/sum(s$d^2)*100,\n     ylab=\"Percent variability explained\")\n\n\n\n\nWe can also make a cumulative plot:\n\n\nplot(cumsum(s$d^2)/sum(s$d^2)*100,\n     ylab=\"Percent variability explained\",\n     ylim=c(0,100), type=\"l\")\n\n\n\n\nAlthough we start with 784 dimensions, we can approximate \\(X\\) with just a few:\n\n\nk <- 100 ## out a possible 784\nYhat <- s$u[,1:k] %*% diag(s$d)[1:k,1:k] %*% t(s$v[,1:k])\nresid <- Y - Yhat\n\n\n\nTherefore, by using only 100 dimensions, we retain most of the variability in our data:\n\n\n1 - var(as.vector(resid))/var(as.vector(Y))\n\n\n[1] 0.9140806\n\nWe say that we explain 91 percent of the variability in our data with 100 PCs.\nNote that we can compute this proportion from \\(\\mathbf{D}\\):\n\n\nsum(s$d[1:k]^2)/sum(s$d^2)\n\n\n[1] 0.9140806\n\nThe entries of \\(\\mathbf{D}\\) therefore tell us how much each PC contributes in term of variability explained.\nAnother way of calculating the PCs is to use prcomp() function.\n\n\npc <- prcomp(X, center=TRUE)\n\n\n\nThe proportion of variance of the first ten PCs is quite high (almost 75%):\n\n\nsummary(pc)$importance[,1:10]\n\n\n                              PC1       PC2       PC3       PC4\nStandard deviation     1136.77538 892.73736 515.18167 470.80915\nProportion of Variance    0.29028   0.17903   0.05962   0.04979\nCumulative Proportion     0.29028   0.46931   0.52893   0.57872\n                             PC5       PC6       PC7       PC8\nStandard deviation     413.27284 391.59029 323.28114 287.81159\nProportion of Variance   0.03837   0.03445   0.02348   0.01861\nCumulative Proportion    0.61708   0.65153   0.67501   0.69361\n                             PC9      PC10\nStandard deviation     246.78152 242.84965\nProportion of Variance   0.01368   0.01325\nCumulative Proportion    0.70729   0.72054\n\nWe can also plot the standard deviations:\n\n\nplot(pc$sdev)\n\n\n\n\nor the more common plot variance explained:\n\n\nplot(pc$sdev^2 / sum(pc$sdev^2))\n\n\n\n\nWe can also see that the first two PCs will in fact be quite informative. Here is a plot of the first two PCs, but colored by the labels that we ignored:\n\n\ndata.frame(PC1 = pc$x[,1], PC2 = pc$x[,2],\n           label=factor(fashion_labels)) %>%\n  ggplot(aes(PC1, PC2, fill=label))+\n  geom_point(cex=3, pch=21)\n\n\n\n\nWe can also “see” the linear combinations on the grid to get an idea of what is getting weighted:\n\n\ntmp <- lapply( c(1:4,781:784), function(i){\n    expand.grid(Row=1:28, Column=1:28) %>%\n      mutate(id=i, label=paste0(\"PC\",i), \n             value = pc$rotation[,i])\n})\ntmp <- Reduce(rbind, tmp)\n\ntmp %>% filter(id<5) %>%\n  ggplot(aes(Row, Column, fill=value)) +\n  geom_raster() +\n  scale_y_reverse() +\n  facet_wrap(~label, nrow = 1)\n\n\n\n\n\n\ntmp %>% filter(id>5) %>%\n  ggplot(aes(Row, Column, fill=value)) +\n  geom_raster() +\n  scale_y_reverse() +\n  facet_wrap(~label, nrow = 1)\n\n\n\n\nData wrangling\nFor purposes of this lecture, we will focus only the 0s (tshirt/top) and 5s (sandals) observations:\n\n\ndat05 <- fashion %>% \n  filter(fashion_labels %in% c(0,5))\n\n## labels are not numbers\ndat05 <- mutate(dat05, label = as.factor(label))\n\n\n\nFeature engineering\nTo distinguish 0s (shirts) from 5s (sandals), it might be enough to look at the number of non-white pixels in the upper-left and lower-right quadrants:\n\n\n\nSo we will define two features \\(X_1\\) and \\(X_2\\) as the percent of non-white pixels in these two quadrants, respectively. We add these two features to the dat05 table\n\n\nrow_column <- expand.grid(row=1:28, col=1:28)\nhead(row_column)\n\n\n  row col\n1   1   1\n2   2   1\n3   3   1\n4   4   1\n5   5   1\n6   6   1\n\n\n\nind1 <- which(row_column$col <= 14 & row_column$row <=14) # top left quandrant\nind2 <- which(row_column$col > 14 & row_column$row > 14) # bottom right quadrant\nind <- c(ind1,ind2)\nX <- as.matrix(dat05[,-1]) # remove label column\nX <- X>200\nX1 <- rowSums(X[,ind1])/rowSums(X)\nX2 <- rowSums(X[,ind2])/rowSums(X)\ndat05 <- mutate(dat05, X_1 = X1, X_2 = X2, \n                y = ifelse(label==\"0\", 0, 1))\n\ndat05 %>% \n  select(label, y, X_1, X_2)\n\n\n# A tibble: 2,000 × 4\n   label     y   X_1   X_2\n   <fct> <dbl> <dbl> <dbl>\n 1 0         0 0.255 0.272\n 2 5         1 0     0.344\n 3 0         0 0.444 0    \n 4 5         1 0     0.409\n 5 5         1 0.153 0.203\n 6 5         1 0     0.362\n 7 0         0 0.364 0.455\n 8 0         0 0.377 0.148\n 9 0         0 0.280 0.244\n10 0         0 0.231 0.263\n# … with 1,990 more rows\n\nExploratory data analysis\nLet’s explore the relationship between the predictors (or features) \\(X_1\\) and \\(X_2\\) and the outcome \\(Y\\):\n\n\ndat05 %>% \n  select(label, X_1, X_2) %>% \n  ggplot(aes(x=label, y=X_1)) + \n  geom_boxplot()\n\n\n\n\nWe see a pronounced relationship between the the predictor \\(X_1\\) and the label (e.g. the \\(X_1\\) feature is high for the t-shirts and low for the sandals, which make sense).\n\n\ndat05 %>% \n  select(label, X_1, X_2) %>% \n  ggplot(aes(x=label, y=X_2)) + \n  geom_boxplot()\n\n\n\n\nIn this case, we again see a difference in the distribution of \\(X_2\\) across the t-shirts and sandals, but less so. This is still likely to be informative.\nFurthermore, we can also plot the relationship between \\(X_1\\) and \\(X_2\\) and see that there is separation between the 0s (t-shirts)and5`s (sandals):\n\n\ndat05 %>% \n  select(label, X_1, X_2) %>% \n  ggplot(aes(x=X_1, y=X_2, color = label)) + \n  geom_point()\n\n\n\n\nCreate train_set and test_set\nIn this last step of data wrangling, we will split the dat05 dataset into two parts:\ntrain_set = the dataset we will use to build the classifer\ntest_set = the dataset we will use to assess how we are doing (not used to train the classifier)\nFor this, we will use the createDataPartition() function in the caret package. We set the seed, so we will all get the same answer\n\n\nset.seed(123)\ninTrain <- createDataPartition(y = dat05$label,\n                               p=0.5)\ntrain_set <- slice(dat05, inTrain$Resample1)\ntest_set <- slice(dat05, -inTrain$Resample1)\n\n\n\nLinear methods for classification\nLinear regression\nOne approach would be to just try using simple linear regression.\nThis assumes that:\n\\[f(x) = \\mbox{Pr}( Y = 1 | X_1=x_1, X_2 = x_2)  = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\]\nand we estimate \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\) with least squares. Once we have estimates \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\), and \\(\\beta_2\\) we can obtain an actual prediction rule:\n\\[ \\hat{f}(x) = \\hat{\\beta}_0+ \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 \\]\nThe problem with using linear regression is that to predict \\(Y\\) on the basis of predictors \\(\\mathbf{X}_i\\), we will need to order the outcomes.\nDoes it make sense here? Not quite. Why?\nIf \\(Y\\) is not quantiative (e.g. ten types of fashion pieces), it doesn’t quite make sense to put a “t-shirt” ahead of a “sandal” or behind a “pants”. This is because to use linear regression, we have to decide on an ordering and if we picked a different ordering, then that coding would produce a fundamentally different linear model with a different set of predictions on the test observations.\nHowever, if the response variable’s values did take on a natural ordering, such as mild, moderate, and severe, and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable.\nUnfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.\nLogistic regression\nIf the qualitative response is binary (0 and 1), then one approach is to fit a linear regression to this binary response and predict 1 if the predicted response (\\(\\hat{Y}\\)) is \\(\\hat{Y} > 0.5\\) or 0 otherwise.\nSo if we consider our linear regression model above:\n\\[f(x) = \\mbox{Pr}( Y = 1 | X_1=x_1, X_2 = x_2)  = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\]\nWe note that the expression on the right can be any real number while the expression on the left is bounded between 0 and 1.\nAn extension that permits us to continue using regression-like models is to apply transformations that eliminate this disconnect. In the case of binary data the most common approach is to fit a logistic regression model which makes use of the logit transformation:\n\\[ g(p) = \\log \\frac{p}{1-p}\\] and use this model instead:\n\\[ g(\\mbox{Pr}(Y=1 \\mid X_1=x_1 , X_2 = x_2) = \n\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\]\nWe can fit a logistic regression model using the glm() function with the family=\"binomial\" argument.\n\n\nfit_glm <- glm(y ~ X_1 + X_2, \n               data = select(train_set, y, X_1, X_2), \n               family = \"binomial\")\nsummary(fit_glm)\n\n\n\nCall:\nglm(formula = y ~ X_1 + X_2, family = \"binomial\", data = select(train_set, \n    y, X_1, X_2))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.7497  -0.4524   0.1068   0.4794   5.3194  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.5549     0.2484   6.260 3.85e-10 ***\nX_1         -15.7030     0.9852 -15.940  < 2e-16 ***\nX_2           2.2024     0.6185   3.561  0.00037 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1386.29  on 999  degrees of freedom\nResidual deviance:  656.62  on 997  degrees of freedom\nAIC: 662.62\n\nNumber of Fisher Scoring iterations: 6\n\nWe see both the \\(X_1\\) and \\(X_2\\) features are statisticially significant at the \\(\\alpha = 0.05\\) level.\nNext, if we predict or classify how each observation \\(Y_i\\) is doing using the predict()\n\n\npred_glm <- predict(fit_glm, newdata = test_set, type=\"response\")\ny_hat_glm <- factor(ifelse(pred_glm > 0.5, 1, 0))\ntab <- table(pred=y_hat_glm, truth= test_set$y)\nconf_matrix <- confusionMatrix(tab)\nconf_matrix\n\n\nConfusion Matrix and Statistics\n\n    truth\npred   0   1\n   0 447  51\n   1  53 449\n                                          \n               Accuracy : 0.896           \n                 95% CI : (0.8754, 0.9142)\n    No Information Rate : 0.5             \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.792           \n                                          \n Mcnemar's Test P-Value : 0.9219          \n                                          \n            Sensitivity : 0.8940          \n            Specificity : 0.8980          \n         Pos Pred Value : 0.8976          \n         Neg Pred Value : 0.8944          \n             Prevalence : 0.5000          \n         Detection Rate : 0.4470          \n   Detection Prevalence : 0.4980          \n      Balanced Accuracy : 0.8960          \n                                          \n       'Positive' Class : 0               \n                                          \n\nThe confusion table can be extracted using the $table slot\n\n\nconf_matrix$table\n\n\n    truth\npred   0   1\n   0 447  51\n   1  53 449\n\nAnd the various performance metrics too:\n\n\nconf_matrix$overall[\"Accuracy\"]\n\n\nAccuracy \n   0.896 \n\nWe can also use the roc() function in the pROC package to plot the ROC curve comparing the sensitivity and specificity\n\n\nroc_glm <- roc(test_set$y, pred_glm)\nplot(roc_glm)\n\n\n\n\n\nLogistic regression for more than 2 response classes\nIn our example, we only considered the t-shirts and sandals, but we technically have more than two classes.\nIf the goal is to classify a response variable or outcome with more than two classes, there are multiple-class extensions.\nHowever, in practice they tend not to be used all that often. One of the reasons is that the method we discuss in the next section, discriminant analysis, is popular for multiple-class classification. So we do not go into the details of multiple-class logistic regression here, but simply note that such an approach is possible, and that software for it is available in R.\n\nLinear discriminant analysis\nLogistic regression involves directly modeling \\(Pr(Y = k|X = x)\\) using the logistic function, for the case of two response classes.\nAn alternative and less direct approach to estimate these probabilities is to model the distribution of the predictors \\(X\\) separately in each of the response classes (i.e. given \\(Y\\)) or \\(Pr(X = x|Y = k)\\), and then use Bayes’ theorem to flip these around into estimates for \\(Pr(Y = k|X = x)\\). Linear discriminant analysis (LDA) assumes these distributions are normal.\nBroadly, LDA finds a linear combination of features that characterizes or separates two or more classes of objects or events.\n\nHow does LDA related to other methods?\nLDA is closely related to ANOVA and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label).\nLogistic regression is more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. In fact, logistic regression preferable in applications where it is not reasonable to assume that the distributions of \\(X|Y\\) are normally distributed.\nLDA is also closely related to PCA in that it looks for linear combinations of variables which best explain the data. However, LDA explicitly attempts to model the difference between the classes of data and PCA does not take into account any difference in class. i.e. LDA needs class labels, but PCA does not.\nLDA works when X are continuous quantities. When dealing with categorical X, the equivalent technique is discriminant correspondence analysis.\n\nWhy do we need another method, when we have logistic regression? There are several reasons:\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suffer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes.\nOk, let’s assume we have \\(K\\) classes (\\(K \\geq2\\)). Let \\(\\pi_k\\) represent the overall or prior probability that a randomly chosen observation comes from the \\(k^{th}\\)class or category of the response variable \\(Y\\).\nLet \\(f_{k}(X) ≡ Pr(X = x|Y = k)\\) denote the density function of \\(X\\) for an observation that comes from the \\(k^{th}\\) class (i.e. \\(f_{k}(x)\\) is relatively large if there is a high probability that an observation in the \\(k^{th}\\) class has \\(X \\approx x\\), and \\(f_{k}(x)\\) is small if it is very unlikely. Then Bayes’ theorem states that\n\\[ \\mbox{Pr}(Y=k|X=x) = \\frac{\\mbox{Pr}(Y=k)  Pr(X = x|Y = k)}{\\sum_{l=1}^K \\mbox{Pr}(Y=l) Pr(X = x|Y = l)} = \\frac{\\pi_k f_{k}(x)}{\\sum_{l=1}^K \\pi_l f_{l}(x)} \\]\nSo instead of directly computing \\(\\mbox{Pr}(Y=1|X=x)\\) (i.e. if \\(K=2\\)) in logistic regression, we can plug in estimates for \\(\\pi_k\\) and \\(f_{k}(x)\\).\nTo do this, we make some assumptions about the distributions of \\(f_{k}(x)\\), namely that they are multivariate normal. LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all \\(K\\) classes. In our case we have two predictors (\\(X_1\\) and \\(X_2\\)), so we assume each one is bivariate normal. This implies we need to estimate two averages, one standard deviation, and a correlation for each case \\(Y=1\\) and \\(Y=0\\).\nThis implies that we can approximate the distributions \\(f_{0}(X_1, X_2)\\) and \\(f_{1}(X_1, X_2)\\). We can easily estimate parameters from the data:\n\n\noptions(digits = 2)\ntrain_set %>% group_by(y) %>% \n  summarize(avg_1 = mean(X_1), \n            avg_2 = mean(X_2))\n\n\n# A tibble: 2 × 3\n      y  avg_1 avg_2\n  <dbl>  <dbl> <dbl>\n1     0 0.303  0.174\n2     1 0.0418 0.377\n\ntrain_set %>% \n  summarize(sd = sd(X_2), \n            r = cor(X_1,X_2))\n\n\n# A tibble: 1 × 2\n     sd      r\n  <dbl>  <dbl>\n1 0.211 -0.575\n\nSo here are the data and contour plots showing the two normal densities:\n\n\ntrain_set %>% mutate(y = factor(y)) %>% \n  ggplot(aes(X_1, X_2, fill = y, color=y)) + \n  geom_point(pch=21, cex=5, color=\"black\") + \n  stat_ellipse(lwd=2, type=\"norm\") \n\n\n\n\nWe use the lda() function in the MASS R package. The prior argument represents the prior probability of class membership.\n\n\nfit_lda <- MASS::lda(y ~ ., \n                     data=dplyr::select(train_set, y, X_1, X_2),\n                     prior = c(1,1)/2)\npred_lda <- MASS:::predict.lda(fit_lda, test_set)$class\n\n\n\n\n\ntab <- table(pred=pred_lda, truth= test_set$y)\nconf_matrix <- confusionMatrix(tab)\nconf_matrix$table\n\n\n    truth\npred   0   1\n   0 425  41\n   1  75 459\n\nconf_matrix$overall[\"Accuracy\"]\n\n\nAccuracy \n    0.88 \n\nQuadratic discriminant analysis\nAs we have discussed, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all \\(K\\) classes.\nQuadratic discriminant analysis (QDA) assumes that the observations from each class are again drawn from a Gaussian distribution and you plug in estimates for the parameters into Bayes’ theorem in order to perform prediction.\nHowever, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the \\(k^{th}\\) class is of the form \\(X \\sim N(\\mu_k,\\Sigma_k)\\), where \\(\\Sigma_k\\) is a covariance matrix for the kth class.\nIn our case, we have two predictors (\\(X_1\\) and \\(X_2\\)), so we assume each one is bivariate normal. This implies we need to estimate two averages, two standard deviations, and a correlation for each case \\(Y=1\\) and \\(Y=0\\).\nThis implies that we can approximate the distributions \\(f_{0}(X_1, X_2)\\) and \\(f_{1}(X_1, X_2)\\). We can easily estimate parameters from the data:\n\n\noptions(digits = 2)\nparams <- train_set %>% group_by(y) %>% \n  summarize(avg_1 = mean(X_1), avg_2 = mean(X_2), \n            sd_1= sd(X_1), sd_2 = sd(X_2), \n            r = cor(X_1,X_2))\nparams\n\n\n# A tibble: 2 × 6\n      y  avg_1 avg_2   sd_1  sd_2      r\n  <dbl>  <dbl> <dbl>  <dbl> <dbl>  <dbl>\n1     0 0.303  0.174 0.166  0.143 -0.599\n2     1 0.0418 0.377 0.0946 0.219 -0.234\n\nSo here are the data and contour plots showing the two normal densities:\n\n\ntrain_set %>% mutate(y = factor(y)) %>% \n  ggplot(aes(X_1, X_2, fill = y, color=y)) + \n  geom_point(pch=21,cex=5, color=\"black\") + \n  stat_ellipse(lwd=2, type=\"norm\")\n\n\n\n\nWe can use the qda() function in the MASS R package. The prior argument represents the prior probability of class membership.\n\n\nfit_qda <- MASS::qda(y ~ ., \n                     data=select(train_set, y, X_1, X_2),\n                     prior = c(1,1)/2)\npred_qda <- MASS:::predict.qda(fit_qda, test_set)$class\n\n\n\n\n\ntab <- table(pred=pred_qda, truth= test_set$y)\nconf_matrix <- confusionMatrix(tab)\nconf_matrix$table\n\n\n    truth\npred   0   1\n   0 438  47\n   1  62 453\n\nconf_matrix$overall[\"Accuracy\"]\n\n\nAccuracy \n    0.89 \n\n\nWhy does it matter if we assume a common covariance matrix?\nIn other words, why would one prefer LDA to QDA, or vice-versa?\nThe answer is the bias-variance trade-off. When there are \\(p\\) predictors, then estimating a covariance matrix requires estimating \\(p(p+1)/2\\) parameters. QDA estimates a separate covariance matrix for each class, for a total of \\(Kp(p+1)/2\\) parameters. With 50 predictors this is some multiple of 1,225, which is a lot of parameters. By instead assuming that the \\(K\\) classes share a common covariance matrix, the LDA model estimates \\(Kp\\) linear coefficients.\nConsequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the \\(K\\) classes share a common covariance matrix is badly off, then LDA can suffer from high bias.\nRoughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the \\(K\\) classes is clearly untenable.\n\n\\(K\\)-nearest neighbors\nA model free alternative is the \\(K\\)-nearest neighbor classifier (KNN). Given a positive integer \\(K\\) and a test observation \\(x_0\\), the KNN classifier first identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(N_0\\). It then estimates the conditional probability for class \\(k\\) as the fraction of points in \\(N_0\\) whose response values equal \\(k\\):\n\\[\\mbox{Pr}( Y = k | \\mathbf{X} =\\mathbf{x}_0)  = \\frac{P(Y = k, \\mathbf{X} = \\mathbf{x}_0)}{P(\\mathbf{X} = \\mathbf{x}_0)} = \\frac{1}{K} \\sum_{i \\in N_0} I(y_i = k)\\]\nFinally, KNN applies Bayes rule and classifies the test observation \\(x_0\\) to the class with the largest probability.\nDespite the fact that it is a very simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier.\n\n\nfit_knn_2 <- knn3(y~., \n                  data = select(train_set, y, X_1, X_2), \n                  k=2)\npred_knn_2 <- predict(fit_knn_2, newdata = test_set)[,2]\n\ntab <- table(pred=round(pred_knn_2), truth= test_set$y)\nconf_matrix <- confusionMatrix(tab)\nconf_matrix$table\n\n\n    truth\npred   0   1\n   0 462  79\n   1  38 421\n\nconf_matrix$overall[\"Accuracy\"]\n\n\nAccuracy \n    0.88 \n\nNote: The choice of \\(K\\) has a drastic effect on the KNN classifier obtained. You can explore this own your own by trying \\(K\\) = 1 vs a larger \\(K\\).\n\n\n\n",
    "preview": "posts/2021-11-10-linear-classification/../../images/pants.jpg",
    "last_modified": "2021-11-10T13:08:00-05:00",
    "input_file": {}
  }
]
